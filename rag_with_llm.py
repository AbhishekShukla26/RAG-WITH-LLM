# -*- coding: utf-8 -*-
"""RAG_WITH_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqwIj0RdJweWOK5l7YoGkeSfOiejjXY2
"""



"""RAG (Retrival Agumented Generation ):
Retrival -> Search knowledge based on user query
Agumented ->
Generator -> Used retrived document as context to generate acurrate relevant response.
LLM hallucination = fluent, confident output that isn’t grounded in truth or provided data.

In our implementation, we will:

Use Wikipedia as our external knowledge source.
Employ Sentence Transformers for embedding text and FAISS for efficient similarity search.
Utilize Hugging Face’s question-answering pipeline to extract answers from retrieved documents.

FAISS stands for Facebook AI Similarity Search. It’s a library used to do fast similarity search and clustering over large sets of vectors (embeddings).

It mostly used to retrive the relevant context for LLM.
"""

pip install transformers

pip install wikipedia

pip install sentence_transformers

!pip install faiss-cpu

import wikipedia
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

"""Retriving Knowledge"""

def get_wikipedia_content(topic):
  try:
    page = wikipedia.page(topic)
    return page.content
  except wikipedia.exceptions.PageError:
    return None
  except wikipedia.exceptions.DisambiguationError as e:
    print(f"Ambiguous topic. Please be more specific. Options: {e.options}")
    return None

# user input
topic = input("Enter a topic to learn about: ")
document = get_wikipedia_content(topic)

if not document:
  print("Could not retrive Info")
  exit()

document

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-mpnet-base-v2")

"""We will split the text into overlapping chucks for better retrival.
Sentence_Transformer -> Hugging Face tokenizer who know how to tokenize the words.[all-mpnet-base-v2]

Here, we are tokenizing the retrieved Wikipedia content and splitting it into smaller overlapping chunks for efficient retrieval. We used a pre-trained tokenizer (all-mpnet-base-v2) to break the text into tokens, then divided it into fixed-size segments (256 tokens each) with an overlap of 20 tokens to maintain context between chunks.
"""

def split_text(text, chunk_size=256, chunk_overlap=20):
    token = tokenizer.tokenize(text)
    chunks = []
    start = 0
    while start < len(token):
      end = min(start+ chunk_size, len(token))
      chunks.append(tokenizer.convert_tokens_to_string(token[start:end]))
      if end == len(token):
         break
      start = end - chunk_overlap
    return chunks

chunks = split_text(document)
print(f"Number of chunks: {len(chunks)}")

"""Step 2: Storing and Retrieving Knowledge
To efficiently search for relevant chunks, we will use Sentence Transformers to convert text into embeddings and store them in a FAISS index:

Here, we converted the text chunks into numerical embeddings using the Sentence Transformer model (all-mpnet-base-v2), which captures their semantic meaning. We then created a FAISS index with an L2 (Euclidean) distance metric and stored the embeddings in it. This will allow us to efficiently retrieve the most relevant chunks based on a user’s query.
"""

embedding_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
embeddings = embedding_model.encode(chunks)

embeddings

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

"""Step 3: Querying the RAG Pipeline
Now, we will take user input for the RAG pipeline. When a user asks a question, we will:

Convert the query into an embedding.
Retrieve the top-k most relevant chunks using FAISS.
Use an LLM-powered question-answering model to generate the answer.
"""

query = input("Ask a question about the topic: ")
query_embedding = embedding_model.encode([query])

k=3
distance, indices = index.search(np.array(query_embedding), k)

distance, indices

retrived_chunks = [chunks[i] for i in indices[0]]
print("Retrived chunks:")
for chunk in retrived_chunks:
  print("-" + chunk)

"""Step 4: Answering the Question with an LLM
Now, we will use a pre-trained question-answering model to extract the final answer from the retrieved context:
"""

qa_model_name = "deepset/roberta-base-squad2"
qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)
qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)
qa_pipeline = pipeline("question-answering",  model=qa_model, tokenizer=qa_tokenizer)

context = " ".join(retrived_chunks)
answer = qa_pipeline(question=query, context=context)
print(f"Answer: {answer['answer']}")
_